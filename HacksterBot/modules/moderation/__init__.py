"""
Moderation Module for HacksterBot.

This module provides comprehensive content moderation capabilities including:
- Text and image content moderation
- URL safety checking
- Automated muting system
- Violation tracking
- AI-powered review system
"""
import logging
import discord
from discord.ext import commands, tasks
from typing import List, Optional, Dict, Any

from core.module_base import ModuleBase
from core.exceptions import ModuleError
from .services.content_moderator import ContentModerator
from .services.url_safety import URLSafetyChecker
from .services.moderation_queue import start_moderation_queue, get_moderation_queue
from .services.moderation_db import ModerationDB
from .services.community_guidelines import format_mute_reason, get_guidelines_for_violations
from .agents.moderation_review import agent_moderation_review, review_flagged_content
from modules.ai.services.ai_select import get_agent

logger = logging.getLogger(__name__)


class ModerationModule(ModuleBase):
    """Content moderation module with AI-powered review system."""
    
    def __init__(self, bot, config):
        """
        Initialize the moderation module.
        
        Args:
            bot: The Discord bot instance
            config: Configuration object
        """
        super().__init__(bot, config)
        self.name = "moderation"
        self.description = "Content moderation with AI review system"
        
        # Initialize services
        self.content_moderator = None
        self.url_safety_checker = None
        self.moderation_db = None
        self.review_agent = None
        self.backup_review_agent = None
        
    async def setup(self):
        """Set up the moderation module."""
        try:
            if not self.config.moderation.enabled:
                logger.info("Moderation module is disabled")
                return
                
            # Initialize content moderator
            self.content_moderator = ContentModerator()
            
            # Initialize URL safety checker
            if self.config.url_safety.enabled:
                self.url_safety_checker = URLSafetyChecker(self.config)
            
            # Initialize database
            self.moderation_db = ModerationDB()
            
            # Start moderation queue
            await start_moderation_queue(self.config)
            
            # Initialize AI review agents if enabled
            if self.config.moderation.enabled:
                await self._setup_review_agents()
            
            # Start background tasks
            self.check_expired_mutes.start()
            
            # Register event listeners
            self.bot.add_listener(self.on_message, 'on_message')
            self.bot.add_listener(self.on_member_join, 'on_member_join')
            
            logger.info("Moderation module setup completed")
            
        except Exception as e:
            logger.error(f"Failed to setup moderation module: {e}")
            raise ModuleError(f"Moderation module setup failed: {e}")
    
    async def teardown(self):
        """Clean up the moderation module."""
        try:
            # Stop background tasks
            if self.check_expired_mutes.is_running():
                self.check_expired_mutes.cancel()
                
            # Close database connections
            if self.moderation_db:
                self.moderation_db.close()
                
            # Remove event listeners
            self.bot.remove_listener(self.on_message, 'on_message')
            self.bot.remove_listener(self.on_member_join, 'on_member_join')
            
            logger.info("Moderation module teardown completed")
            
        except Exception as e:
            logger.error(f"Error during moderation module teardown: {e}")
    
    async def _setup_review_agents(self):
        """Set up AI review agents for moderation."""
        try:
            logger.info("é–‹å§‹è¨­å®š AI è¤‡å¯©ä»£ç†")
            
            # Set up primary review agent
            if self.config.moderation.review_ai_service and self.config.moderation.review_model:
                logger.info(f"è¨­å®šä¸»è¦è¤‡å¯©ä»£ç†: {self.config.moderation.review_ai_service} / {self.config.moderation.review_model}")
                try:
                    primary_model = await get_agent(
                        self.config.moderation.review_ai_service,
                        self.config.moderation.review_model
                    )
                    if primary_model:
                        self.review_agent = await agent_moderation_review(primary_model)
                        logger.info(f"ä¸»è¦è¤‡å¯©ä»£ç†è¨­å®šæˆåŠŸ: {self.config.moderation.review_ai_service}")
                    else:
                        logger.error(f"ç„¡æ³•ç²å–ä¸»è¦ AI ä»£ç†: {self.config.moderation.review_ai_service}")
                except Exception as e:
                    logger.error(f"è¨­å®šä¸»è¦è¤‡å¯©ä»£ç†å¤±æ•—: {e}")
            else:
                logger.warning("ä¸»è¦è¤‡å¯©ä»£ç†é…ç½®ç¼ºå¤±ï¼Œè·³éè¨­å®š")
            
            # Set up backup review agent
            if self.config.moderation.backup_review_ai_service and self.config.moderation.backup_review_model:
                logger.info(f"è¨­å®šå‚™ç”¨è¤‡å¯©ä»£ç†: {self.config.moderation.backup_review_ai_service} / {self.config.moderation.backup_review_model}")
                try:
                    backup_model = await get_agent(
                        self.config.moderation.backup_review_ai_service,
                        self.config.moderation.backup_review_model
                    )
                    if backup_model:
                        self.backup_review_agent = await agent_moderation_review(backup_model)
                        logger.info(f"å‚™ç”¨è¤‡å¯©ä»£ç†è¨­å®šæˆåŠŸ: {self.config.moderation.backup_review_ai_service}")
                    else:
                        logger.error(f"ç„¡æ³•ç²å–å‚™ç”¨ AI ä»£ç†: {self.config.moderation.backup_review_ai_service}")
                except Exception as e:
                    logger.error(f"è¨­å®šå‚™ç”¨è¤‡å¯©ä»£ç†å¤±æ•—: {e}")
            else:
                logger.info("å‚™ç”¨è¤‡å¯©ä»£ç†é…ç½®ç¼ºå¤±ï¼Œè·³éè¨­å®š")
            
            # Log final status
            if self.review_agent or self.backup_review_agent:
                logger.info(f"AI è¤‡å¯©ä»£ç†è¨­å®šå®Œæˆ - ä¸»è¦ä»£ç†: {'æœ‰' if self.review_agent else 'ç„¡'}, å‚™ç”¨ä»£ç†: {'æœ‰' if self.backup_review_agent else 'ç„¡'}")
            else:
                logger.warning("æ‰€æœ‰ AI è¤‡å¯©ä»£ç†è¨­å®šå¤±æ•—")
            
        except Exception as e:
            logger.error(f"è¨­å®šè¤‡å¯©ä»£ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            logger.error(f"éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
    
    async def on_message(self, message: discord.Message):
        """Handle message events for content moderation."""
        # Skip if not enabled or if message is from bot
        if not self.config.moderation.enabled or message.author.bot:
            return
            
        # Skip if author has bypass role
        if hasattr(message.author, 'roles'):
            for role in message.author.roles:
                if role.name in self.config.moderation.bypass_roles:
                    return
        
        try:
            await self._moderate_message(message)
        except Exception as e:
            logger.error(f"Error moderating message {message.id}: {e}")
    
    async def on_member_join(self, member: discord.Member):
        """Handle new member join events."""
        try:
            # Check if member has active timeout
            if self.moderation_db:
                active_mute = self.moderation_db.get_active_mute(member.id, member.guild.id)
                if active_mute:
                    # Reapply timeout if still active
                    import discord
                    expires_at_str = active_mute.get('expires_at')
                    if expires_at_str:
                        from datetime import datetime, timezone
                        expires_at = datetime.fromisoformat(expires_at_str.replace('Z', '+00:00'))
                        if expires_at > discord.utils.utcnow():
                            await member.timeout(expires_at, reason="Reapplying active timeout for rejoining member")
                            logger.info(f"Reapplied timeout to rejoining member {member.id}")
        except Exception as e:
            logger.error(f"Error handling member join {member.id}: {e}")
    
    async def _moderate_message(self, message: discord.Message):
        """Moderate a single message using comprehensive content analysis."""
        # Get message author and content
        author = message.author
        text = message.content.strip()
        attachments = message.attachments
        
        logger.debug(f"é–‹å§‹å¯©æ ¸è¨Šæ¯ {message.id}ï¼Œä½œè€…: {author.display_name}")
        
        # Skip empty messages
        if not text and not attachments:
            return
        
        # Check for URLs if enabled
        url_check_result = None
        if self.url_safety_checker and text:
            try:
                urls = await self.url_safety_checker.extract_urls(text)
                
                if urls:
                    logger.info(f"æª¢æŸ¥ {len(urls)} å€‹ URL åœ¨è¨Šæ¯ä¸­ï¼Œä¾†è‡ª {author.display_name}")
                    is_unsafe, url_results = await self.url_safety_checker.check_urls(urls)
                    
                    # Log the detailed results for each URL
                    for url, result in url_results.items():
                        is_url_unsafe = result.get('is_unsafe', False)
                        try:
                            if is_url_unsafe:
                                threat_types = result.get('threat_types', [])
                                severity = result.get('severity', 0)
                                redirected_to = result.get('redirected_to', None)
                                reason = result.get('reason', '')
                                
                                # Safe joining of threat types
                                threat_types_text = ""
                                try:
                                    threat_types_text = ', '.join(threat_types)
                                except Exception:
                                    threat_types_text = "[æ ¼å¼åŒ–éŒ¯èª¤]"
                                
                                # Format the log message with appropriate error handling
                                if redirected_to:
                                    logger.warning(
                                        f"URLå®‰å…¨æª¢æŸ¥çµæœ: {url} â†’ {redirected_to} | ä¸å®‰å…¨: {is_url_unsafe} | "
                                        f"å¨è„…é¡å‹: {threat_types_text} | åš´é‡åº¦: {severity}" + 
                                        (f" | åŸå› : {reason}" if reason else "")
                                    )
                                else:
                                    logger.warning(
                                        f"URLå®‰å…¨æª¢æŸ¥çµæœ: {url} | ä¸å®‰å…¨: {is_url_unsafe} | "
                                        f"å¨è„…é¡å‹: {threat_types_text} | åš´é‡åº¦: {severity}" + 
                                        (f" | åŸå› : {reason}" if reason else "")
                                    )
                            else:
                                # å°æ–¼å®‰å…¨URLï¼Œè¨˜éŒ„æ›´ç°¡æ½”çš„ä¿¡æ¯
                                message_text = result.get('message', 'å®‰å…¨')
                                logger.info(f"URLå®‰å…¨æª¢æŸ¥çµæœ: {url} | å®‰å…¨ | {message_text}")
                        except Exception as log_error:
                            # Fallback logging if any encoding or formatting errors occur
                            print(f"URL detail logging error: {str(log_error)}")
                            try:
                                logger.info(f"URLå®‰å…¨æª¢æŸ¥çµæœ: [URLè¨˜éŒ„éŒ¯èª¤] | ç‹€æ…‹: {'ä¸å®‰å…¨' if is_url_unsafe else 'å®‰å…¨'}")
                            except:
                                logger.info("URLå®‰å…¨æª¢æŸ¥çµæœè¨˜éŒ„å¤±æ•—")
                    
                    if is_unsafe:
                        # One or more URLs are unsafe
                        unsafe_urls = [url for url, result in url_results.items() if result.get('is_unsafe')]
                        threat_types = set()
                        max_severity = 0
                        reasons = set()
                        
                        for url, result in url_results.items():
                            if result.get('is_unsafe'):
                                # Collect all threat types
                                url_threats = result.get('threat_types', [])
                                for threat in url_threats:
                                    threat_types.add(threat)
                                
                                # Track maximum severity
                                severity = result.get('severity', 0)
                                max_severity = max(max_severity, severity)
                                
                                # Collect reasons if available
                                reason = result.get('reason')
                                if reason:
                                    reasons.add(reason)
                        
                        url_check_result = {
                            "is_unsafe": True,
                            "unsafe_urls": unsafe_urls,
                            "threat_types": list(threat_types),
                            "severity": max_severity,
                            "reasons": list(reasons) if reasons else None,
                            "results": url_results
                        }
                        
                        # Safely join reasons and threat types for logging
                        reason_text = ""
                        if reasons:
                            try:
                                reason_text = f" | åŸå› : {', '.join(reasons)}"
                            except Exception as e:
                                reason_text = " | åŸå› : [æ ¼å¼åŒ–éŒ¯èª¤]"
                                print(f"Reason formatting error: {str(e)}")
                        
                        threat_text = ""
                        if threat_types:
                            try:
                                threat_text = ', '.join(list(threat_types))
                            except Exception as e:
                                threat_text = "[æ ¼å¼åŒ–éŒ¯èª¤]"
                                print(f"Threat type formatting error: {str(e)}")
                        
                        # Safe URL count logging
                        try:
                            url_count = len(urls) if urls else 0
                            unsafe_count = len(unsafe_urls) if unsafe_urls else 0
                            logger.warning(
                                f"URLå®‰å…¨æª¢æŸ¥æ‘˜è¦: ç”¨æˆ¶ {author.display_name}çš„è¨Šæ¯ä¸­æª¢æ¸¬åˆ° "
                                f"{unsafe_count}/{url_count} å€‹ä¸å®‰å…¨URL | å¨è„…é¡å‹: {threat_text}{reason_text}"
                            )
                        except Exception as log_error:
                            # Fallback for any logging errors
                            print(f"URL safety logging error: {str(log_error)}")
                            logger.warning("URLå®‰å…¨æª¢æŸ¥æª¢æ¸¬åˆ°ä¸å®‰å…¨URL (è©³ç´°ä¿¡æ¯è¨˜éŒ„å¤±æ•—)")
                        
                        # If URL is unsafe, we'll continue with deletion and notification below
                        # after both URL and content checks are complete
                    else:
                        logger.info(f"URLå®‰å…¨æª¢æŸ¥æ‘˜è¦: ç”¨æˆ¶ {author.display_name}çš„è¨Šæ¯ä¸­çš„æ‰€æœ‰URL ({len(urls)}å€‹) éƒ½æ˜¯å®‰å…¨çš„")
            except Exception as e:
                logger.error(f"URLå®‰å…¨æª¢æŸ¥éŒ¯èª¤: {str(e)}")

        # Collect all content for moderation
        image_urls = []
        
        # Add attachment URLs
        for attachment in attachments:
            if attachment.content_type and attachment.content_type.startswith('image/'):
                image_urls.append(attachment.url)
        
        # Add image URLs from message content
        if text:
            # Extract image URLs from message content
            import re
            image_url_pattern = r'https?://[^\s<>"]+?\.(?:png|jpg|jpeg|gif|webp)'
            image_urls.extend(re.findall(image_url_pattern, text, re.IGNORECASE))
        
        # Skip if no content to moderate and no unsafe URLs
        if (not text and not image_urls) and not (url_check_result and url_check_result.get('is_unsafe')):
            return
        
        try:
            # Moderate content (text and images)
            is_flagged, results = await self.content_moderator.moderate_content(text, image_urls)
            
            # If either content is flagged or URLs are unsafe, take action
            if is_flagged or (url_check_result and url_check_result.get('is_unsafe')):
                # Save channel and author information before deletion
                channel = message.channel
                guild = message.guild
                
                                # Extract violation categories
                violation_categories = []
                
                # Add URL threat types to violation categories if applicable
                if url_check_result and url_check_result.get('is_unsafe'):
                    for threat_type in url_check_result.get('threat_types', []):
                        violation_category = threat_type.lower()  # Convert PHISHING to phishing, etc.
                        if violation_category not in violation_categories:
                            violation_categories.append(violation_category)
                
                # Check text violations
                if results.get("text_result") and results["text_result"].get("categories"):
                    categories = results["text_result"]["categories"]
                    for category, is_violated in categories.items():
                        if is_violated and category not in violation_categories:
                            violation_categories.append(category)
                
                # Check image violations
                for image_result in results.get("image_results", []):
                    if image_result.get("result") and image_result["result"].get("categories"):
                        categories = image_result["result"]["categories"]
                        for category, is_violated in categories.items():
                            if is_violated and category not in violation_categories:
                                violation_categories.append(category)
                
                logger.warning(f"è¨Šæ¯ {message.id} è¢«æ¨™è¨˜ç‚ºæ½›åœ¨é•è¦ï¼Œé¡åˆ¥: {violation_categories}")
                
                # If review is enabled and this is not a URL safety issue, check if the flagged content is a false positive
                review_result = None
                if (self.config.moderation.review_enabled and text and 
                    (self.review_agent or self.backup_review_agent) and 
                    not (url_check_result and url_check_result.get('is_unsafe'))):
                    from .agents.moderation_review import review_flagged_content
                    
                    try:
                        # Get message context (previous messages)
                        context = ""
                        if hasattr(message.channel, 'history'):
                            context_messages = []
                            async for msg in message.channel.history(limit=self.config.moderation.review_context_messages + 1):
                                if msg.id != message.id:
                                    context_messages.append(f"{msg.author.display_name}: {msg.content}")
                                    if len(context_messages) >= self.config.moderation.review_context_messages:
                                        break
                            
                            if context_messages:
                                context = "æœ€è¿‘çš„è¨Šæ¯ï¼ˆå¾èˆŠåˆ°æ–°ï¼‰ï¼š\n" + "\n".join(reversed(context_messages))
                        
                        # Review the flagged content using OpenAI mod + LLM
                        review_result = await review_flagged_content(
                            agent=self.review_agent,
                            content=text,
                            violation_categories=violation_categories,
                            context=context,
                            backup_agent=self.backup_review_agent
                        )
                        
                        print(f"[å¯©æ ¸ç³»çµ±] ç”¨æˆ¶ {author.display_name} çš„è¨Šæ¯å¯©æ ¸çµæœ: {'éé•è¦(èª¤åˆ¤)' if not review_result['is_violation'] else 'ç¢ºèªé•è¦'}")
                        
                        # If the review agent determined it's a false positive, don't delete or punish
                        if not review_result["is_violation"]:
                            print(f"[å¯©æ ¸ç³»çµ±] èª¤åˆ¤åŸå› : {review_result['reason'][:100]}")
                            # ä¸å°èª¤åˆ¤åšä»»ä½•è™•ç†ï¼Œç›´æ¥è¿”å›
                            return
                            
                    except Exception as review_error:
                        print(f"[å¯©æ ¸ç³»çµ±] åŸ·è¡Œå¯©æ ¸æ™‚å‡ºéŒ¯: {str(review_error)}")
                        # åªæ ¹æ“šé•è¦é¡å‹æ•¸é‡ä¾†æ±ºå®š
                        if len(violation_categories) >= 3:
                            print(f"[å¯©æ ¸ç³»çµ±] è©•ä¼°å¤±æ•—ä½†æª¢æ¸¬åˆ°å¤šç¨®é•è¦é¡å‹ï¼Œè¦–ç‚ºé•è¦")
                            review_result = {
                                "is_violation": True,
                                "reason": f"è©•ä¼°éç¨‹å‡ºéŒ¯ï¼Œä½†å…§å®¹è§¸ç™¼äº†å¤šç¨®é•è¦é¡å‹({', '.join(violation_categories[:3])})ï¼Œç³»çµ±åˆ¤å®šç‚ºé•è¦ã€‚",
                                "original_response": f"ERROR: {str(review_error)}"
                            }
                        # åœ¨å…¶ä»–æƒ…æ³ä¸‹ï¼Œç¹¼çºŒå¸¸è¦å¯©æ ¸æµç¨‹ï¼Œç„¡review_result
                
                # Skip review for URL safety issues - unsafe URLs are always violations
                if url_check_result and url_check_result.get('is_unsafe'):
                    if not review_result:
                        review_result = {
                            "is_violation": True,
                            "reason": f"è¨Šæ¯åŒ…å«ä¸å®‰å…¨çš„é€£çµï¼Œé€™äº›é€£çµå¯èƒ½å«æœ‰è©é¨™ã€é‡£é­šæˆ–æƒ¡æ„è»Ÿé«”å…§å®¹ã€‚",
                            "original_response": "URL_SAFETY_CHECK: Unsafe URLs detected"
                        }
                    elif not review_result.get("is_violation"):
                        # Override non-violation review result for unsafe URLs
                        review_result["is_violation"] = True
                        review_result["reason"] = f"è¨Šæ¯åŒ…å«ä¸å®‰å…¨çš„é€£çµï¼Œé€™äº›é€£çµå¯èƒ½å«æœ‰è©é¨™ã€é‡£é­šæˆ–æƒ¡æ„è»Ÿé«”å…§å®¹ã€‚åŸå§‹å¯©æ ¸çµæœ: {review_result['reason']}"
                        review_result["original_response"] = "URL_SAFETY_CHECK: Unsafe URLs detected"
                
                # Process as violation
                await self._process_violation(message, violation_categories, {
                    'text_result': results.get("text_result"),
                    'image_results': results.get("image_results", []),
                    'url_results': url_check_result,
                    'review_result': review_result
                })
            else:
                logger.debug(f"è¨Šæ¯ {message.id} é€šéæ‰€æœ‰å¯©æ ¸æª¢æŸ¥")
        except Exception as e:
            logger.error(f"å…§å®¹å¯©æ ¸éŒ¯èª¤: {str(e)}")
            import traceback
            logger.error(f"éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
            # Log the error but don't raise, to avoid interrupting normal bot operation
    
    async def _review_flagged_content(self, message: discord.Message, violation_categories: List[str], moderation_result: Dict) -> Dict[str, Any]:
        """Use AI to review flagged content."""
        try:
            logger.info(f"é–‹å§‹ AI è¤‡å¯© - è¨Šæ¯ {message.id}ï¼Œé•è¦é¡åˆ¥: {violation_categories}")
            
            # Get context messages if configured
            context = None
            if self.config.moderation.review_context_messages > 0:
                logger.debug(f"ç²å–ä¸Šä¸‹æ–‡è¨Šæ¯ï¼Œæ•¸é‡: {self.config.moderation.review_context_messages}")
                context_messages = []
                async for msg in message.channel.history(
                    limit=self.config.moderation.review_context_messages,
                    before=message,
                    oldest_first=False
                ):
                    context_messages.append(f"{msg.author.display_name}: {msg.content}")
                
                if context_messages:
                    context = "\n".join(reversed(context_messages))
                    logger.debug(f"å·²ç²å– {len(context_messages)} æ¢ä¸Šä¸‹æ–‡è¨Šæ¯")
                else:
                    logger.debug("æœªæ‰¾åˆ°ä¸Šä¸‹æ–‡è¨Šæ¯")
            
            # Log review details
            logger.info(f"æº–å‚™ç™¼é€çµ¦ AI è¤‡å¯© - å…§å®¹: {message.content[:100]}...")
            logger.debug(f"è¤‡å¯©ä»£ç†ç‹€æ…‹ - ä¸»è¦: {'æœ‰' if self.review_agent else 'ç„¡'}, å‚™ç”¨: {'æœ‰' if self.backup_review_agent else 'ç„¡'}")
            
            # Review the content
            review_result = await review_flagged_content(
                agent=self.review_agent,
                content=message.content,
                violation_categories=violation_categories,
                context=context,
                backup_agent=self.backup_review_agent
            )
            
            logger.info(f"AI è¤‡å¯©å®Œæˆ - è¨Šæ¯ {message.id}ï¼Œçµæœ: {review_result}")
            return review_result
            
        except Exception as e:
            logger.error(f"AI è¤‡å¯©éç¨‹ç™¼ç”ŸéŒ¯èª¤ - è¨Šæ¯ {message.id}: {e}")
            import traceback
            logger.error(f"éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
            # Default to treating as violation on error
            return {"is_violation": True, "reason": "è¤‡å¯©å¤±æ•—ï¼ŒåŸºæ–¼å®‰å…¨è€ƒæ…®è¦–ç‚ºé•è¦"}
    
    async def _process_violation(self, message: discord.Message, violation_categories: List[str], details: Dict):
        """Process a content violation with comprehensive handling."""
        try:
            logger.warning(f"é–‹å§‹è™•ç†é•è¦è¨Šæ¯ {message.id}ï¼Œé¡åˆ¥: {violation_categories}")
            
            # Save channel and author information before deletion
            channel = message.channel
            guild = message.guild
            author = message.author
            text = message.content
            
            # Delete the message (only happens if the review agent confirms it's a violation or review is disabled)
            try:
                review_result = details.get('review_result')
                if review_result is None or review_result.get("is_violation", True):
                    await message.delete()
                    print(f"[å¯©æ ¸ç³»çµ±] å·²åˆªé™¤æ¨™è¨˜ç‚ºé•è¦çš„è¨Šæ¯ï¼Œç”¨æˆ¶: {author.display_name}")
                else:
                    # å¦‚æœè¢«åˆ¤å®šç‚ºèª¤åˆ¤ï¼Œä¸åˆªé™¤æ¶ˆæ¯ä¹Ÿä¸é€šçŸ¥ç”¨æˆ¶
                    print(f"[å¯©æ ¸ç³»çµ±] æ¶ˆæ¯è¢«æ¨™è¨˜ä½†å¯©æ ¸ç¢ºèªç‚ºèª¤åˆ¤ï¼Œå·²ä¿ç•™ã€‚ç”¨æˆ¶: {author.display_name}")
                    return
            except Exception as e:
                print(f"[å¯©æ ¸ç³»çµ±] åˆªé™¤æ¶ˆæ¯å¤±æ•—: {str(e)}")
                return
            
            # Check if user was recently punished - if so, just delete the message without additional notification
            import time
            current_time = time.time()
            user_id = author.id
            is_recent_violator = False
            
            # Simple in-memory tracking for recent violators
            if not hasattr(self, 'tracked_violators'):
                self.tracked_violators = {}
            
            if user_id in self.tracked_violators:
                expiry_time = self.tracked_violators[user_id]
                if current_time < expiry_time:
                    is_recent_violator = True
                    print(f"[å¯©æ ¸ç³»çµ±] ç”¨æˆ¶ {author.display_name} æœ€è¿‘å·²è¢«è™•ç½°ï¼Œåƒ…åˆªé™¤æ¶ˆæ¯è€Œä¸é‡è¤‡è™•ç½°")
                else:
                    # Expired tracking, remove from dictionary
                    del self.tracked_violators[user_id]
            
            # If this is a recent violator, just return after deleting the message
            if is_recent_violator:
                return
                
            # Track this user as a recent violator (24 hours)
            VIOLATION_TRACKING_WINDOW = 24 * 60 * 60  # 24 hours
            self.tracked_violators[user_id] = current_time + VIOLATION_TRACKING_WINDOW
            
            # Record violation in database
            violation_id = self.moderation_db.add_violation(
                user_id=author.id,
                guild_id=guild.id,
                content=text[:1000],  # Limit content length
                violation_categories=violation_categories,
                details=details
            )
            logger.info(f"å·²è¨˜éŒ„é•è¦ {violation_id}ï¼Œç”¨æˆ¶: {author.id}")
            
            # Get violation count
            violation_count = self.moderation_db.get_violation_count(author.id, guild.id)
            logger.info(f"ç”¨æˆ¶ {author.id} é•è¦æ¬¡æ•¸: {violation_count}")
            
            # Apply mute if warranted
            mute_success = False
            mute_reason = ""
            mute_embed = None
            if violation_count >= 1:
                logger.warning(f"æº–å‚™å°ç”¨æˆ¶ {author.id} å¯¦æ–½ç¦è¨€ (é•è¦æ¬¡æ•¸: {violation_count})")
                try:
                    # Add URL safety results if applicable
                    if details.get('url_results') and details['url_results'].get('is_unsafe'):
                        # Update the moderation results to include URL safety results
                        if "url_safety" not in details:
                            details["url_safety"] = details['url_results']
                    
                    mute_success, mute_reason, mute_embed = await self._apply_mute_with_notification(
                        user=author,
                        violation_categories=violation_categories,
                        content=text,
                        details=details,
                        violation_count=violation_count
                    )
                    print(f"[å¯©æ ¸ç³»çµ±] ç”¨æˆ¶ {author.display_name} ç¦è¨€ç‹€æ…‹: {mute_success}")
                except Exception as mute_error:
                    print(f"[å¯©æ ¸ç³»çµ±] ç¦è¨€ç”¨æˆ¶ {author.display_name} æ™‚å‡ºéŒ¯: {str(mute_error)}")
            
            # Clean up old entries in tracked_violators
            if len(self.tracked_violators) > 1000:  # Just to prevent unbounded growth
                current_time = time.time()
                expired_keys = [k for k, v in self.tracked_violators.items() if v < current_time]
                for k in expired_keys:
                    del self.tracked_violators[k]
            
            # Send comprehensive violation notification
            await self._send_comprehensive_violation_notification(
                message, author, channel, guild, violation_categories, violation_count, details, mute_embed
            )
            
            logger.info(f"é•è¦è™•ç†å®Œæˆ - é•è¦ID: {violation_id}ï¼Œç”¨æˆ¶: {author.id}")
            
        except Exception as e:
            logger.error(f"è™•ç†é•è¦è¨Šæ¯ {message.id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            logger.error(f"éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
    

    
    async def _apply_mute_with_notification(self, user: discord.Member, violation_categories: List[str], 
                                           content: str, details: Dict, violation_count: int):
        """Apply Discord timeout to a user and return mute notification embed."""
        try:
            # Calculate mute duration
            duration = self.moderation_db.calculate_mute_duration(violation_count)
            
            if not duration:
                return False, "", None  # No mute needed
            
            # Apply Discord timeout (modern approach, no role needed)
            timeout_until = discord.utils.utcnow() + duration
            await user.timeout(timeout_until, reason=f"Content violation #{violation_count}: {', '.join(violation_categories)}")
            
            # Record mute in database
            self.moderation_db.add_mute(user.id, user.guild.id, violation_count, duration)
            
            logger.info(f"Applied timeout to user {user.id} for {duration}")
            
            # Create mute notification embed
            mute_embed = discord.Embed(
                title="ğŸ”‡ ç¦è¨€é€šçŸ¥",
                description=f"ç”±æ–¼æ‚¨çš„é•è¦è¡Œç‚ºï¼Œæ‚¨å·²è¢«ç¦è¨€ã€‚",
                color=discord.Color.red()
            )
            
            # Format duration
            total_seconds = int(duration.total_seconds())
            if total_seconds >= 86400:  # 1 day
                days = total_seconds // 86400
                mute_embed.add_field(
                    name="ç¦è¨€æ™‚é•·",
                    value=f"{days} å¤©",
                    inline=True
                )
            elif total_seconds >= 3600:  # 1 hour
                hours = total_seconds // 3600
                mute_embed.add_field(
                    name="ç¦è¨€æ™‚é•·",
                    value=f"{hours} å°æ™‚",
                    inline=True
                )
            else:
                minutes = total_seconds // 60
                mute_embed.add_field(
                    name="ç¦è¨€æ™‚é•·",
                    value=f"{minutes} åˆ†é˜",
                    inline=True
                )
            
            # Calculate next violation duration for warning
            next_violation_count = violation_count + 1
            next_duration = self.moderation_db.calculate_mute_duration(next_violation_count)
            if next_duration:
                next_total_seconds = int(next_duration.total_seconds())
                if next_total_seconds >= 86400:
                    next_days = next_total_seconds // 86400
                    next_duration_text = f"{next_days} å¤©"
                elif next_total_seconds >= 3600:
                    next_hours = next_total_seconds // 3600
                    next_duration_text = f"{next_hours} å°æ™‚"
                else:
                    next_minutes = next_total_seconds // 60
                    next_duration_text = f"{next_minutes} åˆ†é˜"
                
                mute_embed.add_field(
                    name="âš ï¸ ä¸‹æ¬¡é•è¦",
                    value=f"ä¸‹æ¬¡é•è¦å°‡è¢«ç¦è¨€ **{next_duration_text}**",
                    inline=False
                )
            
            return True, f"Timed out for {duration}", mute_embed
            
        except Exception as e:
            logger.error(f"Error applying timeout to user {user.id}: {e}")
            import traceback
            logger.error(f"éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
            return False, str(e), None

    async def _send_comprehensive_violation_notification(self, message: discord.Message, author: discord.Member, 
                                                       channel: discord.TextChannel, guild: discord.Guild,
                                                       violation_categories: List[str], violation_count: int, 
                                                       details: Dict, mute_embed: discord.Embed = None):
        """Send comprehensive violation notification based on AIHacker implementation."""
        try:
            import asyncio
            from datetime import datetime, timezone
            
            logger.info(f"é–‹å§‹ç™¼é€é•è¦é€šçŸ¥ - ç”¨æˆ¶: {author.display_name}ï¼Œé•è¦é¡åˆ¥: {violation_categories}")
            
            # Create both embeds then send them simultaneously
            try:
                # Channel notification embed - ç¾è§€ä¸”ç°¡æ½”çš„å…¬å…±é€šçŸ¥
                notification_embed = discord.Embed(
                    title="ğŸ›¡ï¸ å…§å®¹å¯©æ ¸",
                    description=f"<@{author.id}> æ‚¨çš„è¨Šæ¯å› é•åç¤¾ç¾¤è¦ç¯„å·²è¢«ç§»é™¤",
                    color=discord.Color.from_rgb(231, 76, 60)  # æ›´æŸ”å’Œçš„ç´…è‰²
                )
                
                # æ·»åŠ é•è¦é¡å‹çš„ç°¡æ½”é¡¯ç¤º
                if violation_categories:
                    from .services.violation_mapping import get_chinese_category
                    
                    # ç°¡æ½”çš„é•è¦é¡å‹æ˜ å°„ï¼ˆåªç”¨ä¸€å€‹è¡¨æƒ…ç¬¦è™Ÿï¼‰
                    violation_display = {
                        'harassment': 'ğŸš« é¨·æ“¾å…§å®¹',
                        'harassment_threatening': 'âš”ï¸ å¨è„…æ€§é¨·æ“¾',
                        'hate': 'ğŸ’¢ ä»‡æ¨è¨€è«–',
                        'violence': 'ğŸ‘Š æš´åŠ›å…§å®¹',
                        'sexual': 'ğŸ” æ€§ç›¸é—œå…§å®¹',
                        'self-harm': 'ğŸ’” è‡ªæˆ‘å‚·å®³',
                        'self_harm': 'ğŸ’” è‡ªæˆ‘å‚·å®³',
                        'phishing': 'ğŸ£ é‡£é­šç¶²ç«™',
                        'malware': 'ğŸ¦  æƒ¡æ„è»Ÿé«”',
                        'scam': 'ğŸ’° è©é¨™å…§å®¹'
                    }
                    
                    violation_text = []
                    for category in violation_categories[:2]:  # æœ€å¤šé¡¯ç¤º2å€‹ï¼Œé¿å…å¤ªé•·
                        display_text = violation_display.get(category.lower())
                        if not display_text:
                            # å¦‚æœæ²’æœ‰é è¨­çš„ï¼Œä½¿ç”¨ç¿»è­¯
                            chinese_name = get_chinese_category(category)
                            display_text = f"âš ï¸ {chinese_name}"
                        violation_text.append(display_text)
                    
                    if len(violation_categories) > 2:
                        violation_text.append(f"ç­‰ {len(violation_categories)} é …é•è¦")
                    
                    notification_embed.add_field(
                        name="é•è¦é¡å‹",
                        value="\n".join(violation_text),
                        inline=False
                    )
                
                # æ·»åŠ ç¦è¨€ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if mute_embed:
                    duration_field = None
                    for field in mute_embed.fields:
                        if field.name == "ç¦è¨€æ™‚é•·":
                            duration_field = field.value
                            break
                    
                    if duration_field:
                        notification_embed.add_field(
                            name="ğŸ”‡ è™•ç½°",
                            value=f"ç¦è¨€ {duration_field}",
                            inline=True
                        )
                
                notification_embed.add_field(
                    name="ğŸ“¬ è©³ç´°èªªæ˜",
                    value="å·²ç™¼é€è©³ç´°é€šçŸ¥è‡³æ‚¨çš„ç§è¨Š",
                    inline=True
                )
                
                # æ·»åŠ æ™‚é–“æˆ³å’Œå°å­—æç¤º
                notification_embed.timestamp = datetime.now(timezone.utc)
                notification_embed.set_footer(text=f"æ­¤è¨Šæ¯å°‡åœ¨ {self.config.moderation.notification_timeout} ç§’å¾Œè‡ªå‹•åˆªé™¤")
                
                # DM embed
                dm_embed = discord.Embed(
                    title="ğŸ›¡ï¸ å…§å®¹å¯©æ ¸é€šçŸ¥",
                    description=f"æ‚¨åœ¨ **{guild.name}** ç™¼é€çš„è¨Šæ¯å› å«æœ‰ä¸é©ç•¶å…§å®¹è€Œè¢«ç§»é™¤ã€‚",
                    color=discord.Color.from_rgb(230, 126, 34)  # Warm orange color
                )
                
                # Add server icon if available
                if guild.icon:
                    dm_embed.set_thumbnail(url=guild.icon.url)
                
                dm_embed.timestamp = datetime.now(timezone.utc)
                
                # Add URL safety information if applicable
                url_results = details.get('url_results')
                if url_results and url_results.get('is_unsafe'):
                    unsafe_urls = url_results.get('unsafe_urls', [])
                    if unsafe_urls:
                        url_list = "\n".join([f"- {url}" for url in unsafe_urls[:5]])  # Limit to 5 URLs
                        if len(unsafe_urls) > 5:
                            url_list += f"\n- ...ä»¥åŠ {len(unsafe_urls) - 5} å€‹å…¶ä»–ä¸å®‰å…¨é€£çµ"
                            
                        threat_types_map = {
                            'PHISHING': 'é‡£é­šç¶²ç«™',
                            'MALWARE': 'æƒ¡æ„è»Ÿé«”',
                            'SCAM': 'è©é¨™ç¶²ç«™',
                            'SUSPICIOUS': 'å¯ç–‘ç¶²ç«™'
                        }
                        
                        threat_descriptions = []
                        for threat in url_results.get('threat_types', []):
                            threat_descriptions.append(threat_types_map.get(threat, threat))
                            
                        threat_text = "ã€".join(threat_descriptions) if threat_descriptions else "ä¸å®‰å…¨é€£çµ"
                        
                        dm_embed.add_field(
                            name="âš ï¸ ä¸å®‰å…¨é€£çµ",
                            value=f"æ‚¨çš„è¨Šæ¯åŒ…å«å¯èƒ½æ˜¯{threat_text}çš„é€£çµï¼š\n{url_list}",
                            inline=False
                        )
                
                # Add violation types with emoji indicators and Chinese translations
                if violation_categories:
                    from .services.violation_mapping import get_chinese_category, get_chinese_description
                    
                    violation_list = []
                    for category in violation_categories:
                        category_text = get_chinese_category(category)
                        violation_list.append(category_text)
                    
                    dm_embed.add_field(
                        name="é•è¦é¡å‹",
                        value="\n".join(violation_list),
                        inline=False
                    )
                
                # Add channel information
                dm_embed.add_field(
                    name="ğŸ“ é »é“",
                    value=f"#{channel.name}",
                    inline=True
                )
                
                # Add violation count
                dm_embed.add_field(
                    name="ğŸ”¢ é•è¦æ¬¡æ•¸",
                    value=f"é€™æ˜¯æ‚¨çš„ç¬¬ **{violation_count}** æ¬¡é•è¦",
                    inline=True
                )
                
                # Add a divider
                dm_embed.add_field(
                    name="",
                    value="â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
                    inline=False
                )
                
                # Add the original content that was flagged
                if message.content:
                    # Truncate text if too long
                    display_text = message.content if len(message.content) <= 1000 else message.content[:997] + "..."
                    dm_embed.add_field(
                        name="ğŸ“„ è¨Šæ¯å…§å®¹",
                        value=f"```\n{display_text}\n```",
                        inline=False
                    )
                
                if message.attachments:
                    dm_embed.add_field(
                        name="ğŸ–¼ï¸ é™„ä»¶",
                        value=f"åŒ…å« {len(message.attachments)} å€‹é™„ä»¶",
                        inline=False
                    )
                
                # Add another divider
                dm_embed.add_field(
                    name="",
                    value="â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
                    inline=False
                )
                
                # Add note and resources
                dm_embed.add_field(
                    name="ğŸ“‹ è«‹æ³¨æ„",
                    value="è«‹ç¢ºä¿æ‚¨ç™¼é€çš„å…§å®¹ç¬¦åˆç¤¾ç¾¤è¦ç¯„ã€‚é‡è¤‡é•è¦å¯èƒ½å°è‡´æ›´åš´é‡çš„è™•ç½°ã€‚\n\nå¦‚æœæ‚¨å°æ­¤æ±ºå®šæœ‰ç–‘å•ï¼Œè«‹è¯ç¹«ä¼ºæœå™¨å·¥ä½œäººå“¡ã€‚",
                    inline=False
                )
                
                # Add guidelines link
                dm_embed.add_field(
                    name="ğŸ“š ç¤¾ç¾¤è¦ç¯„",
                    value=f"è«‹é–±è®€æˆ‘å€‘çš„[ç¤¾ç¾¤è¦ç¯„](https://discord.com/channels/{guild.id}/rules)ä»¥äº†è§£æ›´å¤šè³‡è¨Šã€‚",
                    inline=False
                )
                
                # Send both messages simultaneously
                tasks = []
                tasks.append(channel.send(embed=notification_embed))
                tasks.append(author.send(embed=dm_embed))
                
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Log any DM errors
                if len(results) > 1 and isinstance(results[1], Exception):
                    print(f"Failed to send DM: {str(results[1])}")
                
                # Send mute notification after content moderation notification
                if mute_embed:
                    try:
                        await author.send(embed=mute_embed)
                    except Exception as e:
                        print(f"Failed to send mute notification DM: {str(e)}")

                # Extract channel notification for deletion
                if len(results) > 0 and isinstance(results[0], discord.Message):
                    channel_notification = results[0]
                    # Delete the notification after a short delay
                    await asyncio.sleep(self.config.moderation.notification_timeout)
                    try:
                        await channel_notification.delete()
                    except:
                        pass  # Ignore deletion errors
                
            except Exception as e:
                print(f"Failed to send notification messages: {str(e)}")
                
            logger.info(f"é•è¦é€šçŸ¥è™•ç†å®Œæˆ - ç”¨æˆ¶: {author.display_name}")
            
        except Exception as e:
            logger.error(f"ç™¼é€é•è¦é€šçŸ¥æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            logger.error(f"éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
    
    @tasks.loop(minutes=5)
    async def check_expired_mutes(self):
        """Check for and remove expired timeouts."""
        try:
            expired_mutes = self.moderation_db.check_and_update_expired_mutes()
            
            for mute_data in expired_mutes:
                try:
                    guild = self.bot.get_guild(mute_data['guild_id'])
                    if not guild:
                        continue
                    
                    member = guild.get_member(mute_data['user_id'])
                    if not member:
                        continue
                    
                    # Check if member still has timeout and remove it
                    if member.timed_out_until:
                        await member.timeout(None, reason="Timeout expired")
                        logger.info(f"Removed expired timeout from user {member.id}")
                        
                except Exception as e:
                    logger.error(f"Error removing expired timeout: {e}")
                    
        except Exception as e:
            logger.error(f"Error checking expired timeouts: {e}")
    
    @check_expired_mutes.before_loop
    async def before_check_expired_mutes(self):
        """Wait until bot is ready before starting the task."""
        await self.bot.wait_until_ready()


def setup(bot, config):
    """Set up the moderation module."""
    return ModerationModule(bot, config) 